{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import logging\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "# Data handling and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn libraries for preprocessing and metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Scikit-learn libraries for model selection\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate, train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "# Scikit-learn libraries for models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Scikit-learn libraries for pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Imbalanced-learn libraries\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(scaler_type: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \n",
    "    # Pobranie danych z plików .csv\n",
    "    X = pd.read_csv('wineData.csv')\n",
    "    y = pd.read_csv('wineRatings.csv')\n",
    "    y = y.values.ravel()\n",
    "    \n",
    "    #Informacje o danych do logów\n",
    "    logging.info(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    #Przeskalowanie danych przy użyciu obiektu MinMaxScaler\n",
    "    if scaler_type in [\"minmax\", \"standard\"]:\n",
    "        scaler = MinMaxScaler() if scaler_type == \"minmax\" else StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        logging.info(f\"Data scaled using {scaler_type} scaler.\")\n",
    "    else:\n",
    "        logging.info(\"No scaling applied.\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja definiująca strategię próbkowania\n",
    "def sampling_strategy(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    return {class_label: max(int(0.40 * max_count), count) for class_label, count in zip(unique, counts)}\n",
    "\n",
    "#Funkcja dzieląca dane na zbiór treningowy i testowy oraz nakladająca oversampler\n",
    "def initialize_and_split_data(scaler_type: str, method: str = 'none') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X, y = initialize_data(scaler_type)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    if method == 'smote':\n",
    "        strategy = sampling_strategy(y_train)\n",
    "        oversampler = SMOTE(sampling_strategy=strategy, k_neighbors=2, random_state=42)\n",
    "        X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "        logging.info(f\"Applied {method} to handle class imbalance in the training data.\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#Funkcja ewaluująca model\n",
    "def cross_validation_and_evaluation(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any], cv_splits: int = 5, random_state: int = 42) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, list, Dict[str, Any]]:\n",
    "    # Set the best parameters for the model\n",
    "    model.set_params(**best_params)\n",
    "    pipe = make_pipeline(model)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    kfold = KFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, cv=kfold, return_train_score=True)\n",
    "    logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "\n",
    "\n",
    "    logging.info(f\"Mean accuracy score over all folds: {cv_results['test_score'].mean() * 100:.2f}%\")\n",
    "    logging.info(f\"Mean training accuracy score over all folds: {cv_results['train_score'].mean() * 100:.2f}%\")\n",
    "    return cv_results, cv_results['test_score'].mean()\n",
    "\n",
    "#Funkcja znajująca najlepsze hiperparametry\n",
    "def optimize_hyperparameters(estimator, X_train: np.ndarray, y_train: np.ndarray, param_grid: Dict[str, Any], n_iter: int = 10, cv_splits: int = 5, random_state: int = 42) -> Dict[str, Any]:\n",
    "    random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=n_iter, cv= cv_splits, scoring='accuracy', random_state=random_state, n_jobs=-1, verbose=2)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    logging.info(f\"Best accuracy score: {random_search.best_score_ * 100:.2f}%\")\n",
    "    logging.info(f\"Best parameters: {random_search.best_params_}\")\n",
    "    return random_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja definiująca strategię próbkowania\n",
    "def sampling_strategy(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    return {class_label: max(int(0.40 * max_count), count) for class_label, count in zip(unique, counts)}\n",
    "\n",
    "#Funkcja dzieląca dane na zbiór treningowy i testowy oraz nakladająca oversampler\n",
    "def initialize_and_split_data(scaler_type: str, method: str = 'none') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X, y = initialize_data(scaler_type)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    if method == 'smote':\n",
    "        strategy = sampling_strategy(y_train)\n",
    "        oversampler = SMOTE(sampling_strategy=strategy, k_neighbors=2, random_state=42)\n",
    "        X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "        logging.info(f\"Applied {method} to handle class imbalance in the training data.\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#Funkcja ewaluująca model\n",
    "def cross_validation_and_evaluation(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any], cv_splits: int = 5, random_state: int = 42) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, list, Dict[str, Any]]:\n",
    "    # Set the best parameters for the model\n",
    "    model.set_params(**best_params)\n",
    "    pipe = make_pipeline(model)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    kfold = KFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, cv=kfold, return_train_score=True)\n",
    "    logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "\n",
    "\n",
    "    logging.info(f\"Mean accuracy score over all folds: {cv_results['test_score'].mean() * 100:.2f}%\")\n",
    "    logging.info(f\"Mean training accuracy score over all folds: {cv_results['train_score'].mean() * 100:.2f}%\")\n",
    "    return cv_results, cv_results['test_score'].mean()\n",
    "\n",
    "#Funkcja znajująca najlepsze hiperparametry\n",
    "def optimize_hyperparameters(estimator, X_train: np.ndarray, y_train: np.ndarray, param_grid: Dict[str, Any], n_iter: int = 100, cv_splits: int = 5, random_state: int = 42) -> Dict[str, Any]:\n",
    "    random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=n_iter, cv= cv_splits, scoring='accuracy', random_state=random_state, n_jobs=-1, verbose=2)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    logging.info(f\"Best accuracy score: {random_search.best_score_ * 100:.2f}%\")\n",
    "    logging.info(f\"Best parameters: {random_search.best_params_}\")\n",
    "    return random_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(y_test: Any, y_pred: Any) -> Dict[str, Any]:\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
    "    logging.info(\"Classification report:\\n%s\", classification_report(y_test, y_pred, zero_division=1))\n",
    "    logging.info(\"Confusion matrix:\\n%s\", confusion_matrix(y_test, y_pred))\n",
    "    return report\n",
    "\n",
    "#Funkcja generująca histogram\n",
    "def plot_histogram(y_test: Any, y_pred: Any, ax: plt.Axes) -> None:\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).plot.hist(ax=ax, alpha=0.5)\n",
    "    ax.set_title('Actual vs Predicted values')\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend(['Actual', 'Predicted'])\n",
    "\n",
    "#Funkcja generująca wykres przewidywań vs właściwych ocen wina\n",
    "def plot_actual_vs_predicted(y_test: Any, y_pred: Any, ax: plt.Axes) -> None:\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).plot(ax=ax, marker='o')\n",
    "    ax.set_title('Actual vs Predicted values')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.legend(['Actual', 'Predicted'])\n",
    "\n",
    "#Funkcja generująca raport klasyfikacji\n",
    "def plot_classification_report(report: Dict[str, Any], ax: plt.Axes) -> None:\n",
    "    report_df = pd.DataFrame(report).transpose().drop(columns='support')\n",
    "    report_df.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Classification Report Metrics')\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "#Funkcja generująca macierz pomyłek\n",
    "def plot_confusion_matrix(cm, classes, ax: plt.Axes) -> None:\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "#Funkcja wywołująca wszystkie wykesy\n",
    "def display_plots(y_fold_test: np.ndarray, y_pred: np.ndarray, report: Dict[str, Any], cv_results: Dict[str, Any], accuracy_without_optimization: float, accuracy_with_optimization: float) -> None:\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    classes = [3, 4, 5, 6, 7, 8]\n",
    "    plot_actual_vs_predicted(y_fold_test, y_pred, axs[0, 0])\n",
    "    plot_confusion_matrix(confusion_matrix(y_fold_test, y_pred), classes, axs[0, 1])\n",
    "    plot_classification_report(report, axs[1, 0])\n",
    "    plot_histogram(y_fold_test, y_pred, axs[2, 1])\n",
    "    \n",
    "    axs[1, 1].plot(cv_results['test_score'], label='Test Score')\n",
    "    axs[1, 1].plot(cv_results['train_score'], label='Train Score')\n",
    "    axs[1, 1].set_title('Cross-validation Results with Optimized Hyperparameters')\n",
    "    axs[1, 1].set_xlabel('Fold Index')\n",
    "    axs[1, 1].set_ylabel('Accuracy')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[2, 0].bar(['Without Optimization', 'With Optimization'], [accuracy_without_optimization, accuracy_with_optimization])\n",
    "    axs[2, 0].set_title('Comparison of Model Performance')\n",
    "    axs[2, 0].set_ylabel('Mean Accuracy Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 12:04:54,327 - INFO - X shape: (1503, 5), y shape: (1503,)\n",
      "2024-06-01 12:04:54,333 - INFO - Data scaled using standard scaler.\n",
      "2024-06-01 12:04:54,335 - INFO - Data initialization and splitting complete.\n",
      "2024-06-01 12:04:56,097 - INFO - Cross-validation results: {'fit_time': array([0.268646  , 0.24221039, 0.31908131, 0.23784757, 0.263098  ]), 'score_time': array([0.01790142, 0.01791239, 0.01971984, 0.02597976, 0.02046156]), 'test_score': array([0.61410788, 0.59751037, 0.6625    , 0.65      , 0.62083333]), 'train_score': array([0.82726327, 0.81373569, 0.80769231, 0.8045738 , 0.8045738 ])}\n",
      "2024-06-01 12:04:56,097 - INFO - Mean accuracy score over all folds: 62.90%\n",
      "2024-06-01 12:04:56,097 - INFO - Mean training accuracy score over all folds: 81.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ensemble_main():\n",
    "    try:\n",
    "        #Podział danych na zbior testowy i treningowy\n",
    "        X_train, X_test, y_train, y_test = initialize_and_split_data(\"standard\", 'none')\n",
    "        logging.info(\"Data initialization and splitting complete.\")\n",
    "\n",
    "        #Utworzenie obiektów modeli\n",
    "        clf1 = GaussianNB()\n",
    "        clf2 = RandomForestClassifier()\n",
    "        clf3 = KNeighborsClassifier()\n",
    "\n",
    "        #Utworzenie obiektu modelu ensemble\n",
    "        eclf = VotingClassifier(estimators=[('gnb', clf1), ('rf', clf2), ('knn', clf3)], voting='hard')\n",
    "\n",
    "        #Wytrenowanie i wstepna ocena modelu\n",
    "        best_params = {}\n",
    "        cv_results, accuracy_without_optimization = cross_validation_and_evaluation(X_train, y_train, eclf, best_params)\n",
    "\n",
    "        #Hiperparametryzacja\n",
    "        # Note: You need to specify the hyperparameters for each classifier separately\n",
    "        param_grid = {\n",
    "            'gnb__var_smoothing': np.logspace(0, -9, num=100),\n",
    "            'rf__n_estimators': [50, 100, 200],\n",
    "            'knn__n_neighbors': [3, 5, 7]\n",
    "        }\n",
    "        best_params = optimize_hyperparameters(eclf, X_train, y_train, param_grid)\n",
    "\n",
    "        #Walidacja krzyżowa modelu ze zoptymalizowanymi hiperparametrami\n",
    "        cv_resultsOPT, accuracy_with_optimization = cross_validation_and_evaluation(X_train, y_train, eclf, best_params)\n",
    "\n",
    "        #Ponowne wyktrenowanie modelu, ale ze zoptymalizowanymi hiperparametrami\n",
    "        eclf.set_params(**best_params)\n",
    "        eclf.fit(X_train, y_train)\n",
    "\n",
    "        #Predykcje na zbiorze testowym\n",
    "        y_pred_optimized = eclf.predict(X_test)\n",
    "\n",
    "        #Generacja raportu dla modelu\n",
    "        report_optimized = generate_report(y_test, y_pred_optimized)\n",
    "\n",
    "        #Generacja wykresów dla modelu\n",
    "        display_plots(y_test, y_pred_optimized, report_optimized, cv_resultsOPT, accuracy_without_optimization, accuracy_with_optimization)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during model evaluation and plotting.\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
