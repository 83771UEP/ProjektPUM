{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "█▄▀ █░░ ▄▀█ █▀ █▄█ █▀▀ █ █▄▀ ▄▀█ ▀█▀ █▀█ █▀█   █▄▄ ▄▀█ ░░█ █▀▀ █▀ █▀█ █░█░█ █▀ █▄▀ █   \n",
    "█░█ █▄▄ █▀█ ▄█ ░█░ █▀░ █ █░█ █▀█ ░█░ █▄█ █▀▄   █▄█ █▀█ █▄█ ██▄ ▄█ █▄█ ▀▄▀▄▀ ▄█ █░█ █   \n",
    "\n",
    "▀█▀ █░█ █▀▀   █▀▀ █▀█ █▀▀ ▄▀█ ▀█▀   █▀▄▀█ █▀█ █▀▀   █▀▀ █▀▄ █ ▀█▀ █ █▀█ █▄░█\n",
    "░█░ █▀█ ██▄   █▄█ █▀▄ ██▄ █▀█ ░█░   █░▀░█ █▄█ █▄█   ██▄ █▄▀ █ ░█░ █ █▄█ █░▀█ \n",
    "(AKA I copied it from someone smarter than both me and chatGPT vomit edition)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fajny artykuł na którym oparłem dużą część tego projektu:\n",
    "Artykuł \"Naive Bayes and Hyperparameter Optimization\" przedstawia w sposób przystępny podstawy klasyfikatora Naive Bayes oraz techniki optymalizacji hiperparametrów. Obejmuje zarówno teoretyczne aspekty, jak i praktyczne przykłady implementacji w Pythonie, co czyni go wartościowym źródłem wiedzy dla osób zainteresowanych uczeniem maszynowym.\n",
    "\n",
    "Link do artykułu: [Naive Bayes and Hyperparameter Optimization](https://bait509-ubc.github.io/BAIT509/lectures/lecture6.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalety i wady naiwnego klasyfikatora Bajesowskiego w kontekście naszego projektu\n",
    "\n",
    "**Zalety**\n",
    "\n",
    "- **Mniejsza złożoność:** Naïve Bayes jest prostszy w porównaniu do innych klasyfikatorów. \n",
    "- **Dobra skalowalność:** Jest szybki i efektywny, a także wymaga niewielkiej ilości pamięci.\n",
    "- **Obsługa danych o wysokiej wymiarowości:** Sprawdza się w przypadkach takich jak klasyfikacja dokumentów, gdzie liczba wymiarów jest wysoka.\n",
    "\n",
    "**Wady**\n",
    "\n",
    "- **Zjawisko zerowej częstości:** Występuje, gdy zmienna kategoryczna nie istnieje w zbiorze treningowym, co prowadzi do zerowego prawdopodobieństwa warunkowego. Problem ten można rozwiązać za pomocą wygładzania Laplace’a.\n",
    "- **Nierealistyczne założenie podstawowe:** Założenie o niezależności warunkowej nie zawsze jest spełnione, co może prowadzić do błędnych klasyfikacji.\n",
    "\n",
    "**Zastosowania klasyfikatora Naïve Bayes**\n",
    "\n",
    "- **Filtrowanie spamu:** Jedno z najpopularniejszych zastosowań, gdzie Naïve Bayes służy do klasyfikacji wiadomości jako spam.\n",
    "- **Klasyfikacja dokumentów:** Używany do klasyfikacji treści, np. artykułów na stronach mediów informacyjnych.\n",
    "- **Analiza sentymentu:** Pomaga w zrozumieniu opinii i postaw wobec produktów i marek.\n",
    "- **Przewidywanie stanów umysłowych:** Wykorzystywany w analizie danych fMRI do przewidywania stanów poznawczych ludzi.\n",
    "źródło IBM\n",
    "\n",
    "\n",
    "| Nazwa klasyfikatora      | Opis                                                                                       | Założenie                                                     | Nadaje się do                                                    | Przyjęto (wyjaśnienie)    |\n",
    "|--------------------------|--------------------------------------------------------------------------------------------|---------------------------------------------------------------|------------------------------------------------------------------|--------------------------|\n",
    "| Gaussian Naive Bayes     | Zakłada normalny (gaussowski) rozkład cech w każdej klasie.                                | Cechy mają ciągły rozkład gaussowski.                         | Numeryczne cechy, które są w przybliżeniu normalnie rozłożone.    |✅|\n",
    "| Multinomial Naive Bayes  | Zaprojektowany dla dyskretnych danych opartych na liczeniu (np. dane tekstowe reprezentowane przez częstotliwość słów). | Cechy są liczbami wystąpień w różnych klasach.                 | Klasyfikacja tekstów, kategoryzacja dokumentów, analiza sentymentu. |❌✅|\n",
    "| Complement Naive Bayes   | Zakłada cechy binarne lub logiczne, wskazujące obecność lub brak cechy.                     | Cechy są binarne (0/1).                                        | Klasyfikacja tekstów, klasyfikacja danych binarnych.              |❌|\n",
    "| Categorical Naive Bayes  | Obsługuje cechy kategoryczne, które mogą przyjmować dyskretne wartości bez zakładania żadnego określonego rozkładu. | Cechy są zmiennymi kategorycznymi.                             | Klasyfikacja tekstów, systemy rekomendacyjne z danymi kategorycznymi. |✅|\n",
    "| Bernoulli Naive Bayes    | Zakłada cechy binarne lub logiczne, wskazujące obecność lub brak cechy.                     | Cechy są binarne (0/1).                                        | Klasyfikacja tekstów, klasyfikacja danych binarnych.              |❌|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the Data Science Gang in da hood \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate, train_test_split, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#debugging stuff\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Initialization Documentation\n",
    "## `initialize_data`\n",
    "Initializes and preprocesses the data.\n",
    "### Parameters:\n",
    "- `scaler_type` (str, optional): The type of scaler to use for preprocessing. \n",
    "  - Options are `'minmax'` or `'standard'`. \n",
    "  - If `None`, no scaling is applied.\n",
    "### Returns:\n",
    "- `Tuple[pd.DataFrame, pd.Series]`: The preprocessed feature data and the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(scaler_type: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    # Load data from CSV files\n",
    "    X = pd.read_csv('wineData.csv')\n",
    "    y = pd.read_csv('wineRatings.csv')\n",
    "    y = y.values.ravel()\n",
    "    \n",
    "    # Log data information\n",
    "    logging.info(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    # Preprocess data using the specified scaler\n",
    "    if scaler_type in [\"minmax\", \"standard\"]:\n",
    "        scaler = MinMaxScaler() if scaler_type == \"minmax\" else StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        logging.info(f\"Data scaled using {scaler_type} scaler.\")\n",
    "    else:\n",
    "        logging.info(\"No scaling applied.\")\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions\n",
    "\n",
    "# Function Documentation\n",
    "\n",
    "## `evaluate_model`\n",
    "\n",
    "Evaluates a model using K-Fold Cross Validation.\n",
    "\n",
    "### Parameters:\n",
    "- `X_train` (np.ndarray): Training feature data.\n",
    "- `y_train` (np.ndarray): Training target data.\n",
    "- `model` (Any): The machine learning model to evaluate.\n",
    "- `cv_splits` (int): Number of cross-validation splits.\n",
    "- `random_state` (int): Random state for reproducibility.\n",
    "\n",
    "### Returns:\n",
    "- `Tuple[float, np.ndarray, np.ndarray, np.ndarray, list]`: Mean accuracy, last fold test data, last fold test targets, last fold predictions, list of accuracy scores for each fold.\n",
    "\n",
    "## `optimize_hyperparameters`\n",
    "\n",
    "Optimizes hyperparameters using RandomizedSearchCV.\n",
    "\n",
    "### Parameters:\n",
    "- `X_train` (np.ndarray): Training feature data.\n",
    "- `y_train` (np.ndarray): Training target data.\n",
    "- `estimator` (Any): The machine learning model to optimize.\n",
    "- `param_grid` (Dict[str, Any]): The parameter grid to search.\n",
    "- `n_iter` (int): Number of parameter settings sampled.\n",
    "- `cv_splits` (int): Number of cross-validation splits.\n",
    "- `random_state` (int): Random state for reproducibility.\n",
    "\n",
    "### Returns:\n",
    "- `Dict[str, Any]`: Best parameters found by RandomizedSearchCV.\n",
    "\n",
    "## `optimize_hyperparameters_mb`\n",
    "\n",
    "Optimizes hyperparameters for MultinomialNB using RandomizedSearchCV.\n",
    "\n",
    "### Parameters:\n",
    "- `X_train` (np.ndarray): Training feature data.\n",
    "- `y_train` (np.ndarray): Training target data.\n",
    "- `param_grid` (Dict[str, Any]): The parameter grid to search.\n",
    "- `random_state` (int): Random state for reproducibility.\n",
    "- `estimator` (Any): The machine learning model to optimize.\n",
    "- `n_iter` (int): Number of parameter settings sampled.\n",
    "\n",
    "### Returns:\n",
    "- `Dict[str, Any]`: Best parameters found by RandomizedSearchCV.\n",
    "\n",
    "## `cross_validation_with_optimized_hyperparameters`\n",
    "\n",
    "Performs cross-validation with optimized hyperparameters.\n",
    "\n",
    "### Parameters:\n",
    "- `X_train` (np.ndarray): Training feature data.\n",
    "- `y_train` (np.ndarray): Training target data.\n",
    "- `model` (Any): The machine learning model to evaluate.\n",
    "- `best_params` (Dict[str, Any]): Best parameters found by hyperparameter optimization.\n",
    "- `cv_splits` (int): Number of cross-validation splits.\n",
    "- `random_state` (int): Random state for reproducibility.\n",
    "- `model_name` (str): The name of the model ('GaussianNB' or 'MultinomialNB').\n",
    "\n",
    "### Returns:\n",
    "- `Dict[str, Any]`: Cross-validation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train: np.ndarray, y_train: np.ndarray, model: Any, cv_splits: int = 10, random_state: int = 2137) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, list]:\n",
    "    kfold = KFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train[train_index], X_train[test_index]\n",
    "        y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred = model.predict(X_fold_test)\n",
    "        accuracy = accuracy_score(y_fold_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        logging.info(f\"Accuracy for this fold: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    mean_accuracy = np.mean(accuracy_scores)\n",
    "    return mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores\n",
    "\n",
    "def optimize_hyperparameters(X_train: np.ndarray, y_train: np.ndarray, estimator: Any, param_grid: Dict[str, Any], n_iter: int = 10, cv_splits: int = 5, random_state: int = 42) -> Dict[str, Any]:\n",
    "    random_search = RandomizedSearchCV(estimator, param_distributions=param_grid, n_iter=n_iter, cv=cv_splits, scoring='accuracy', random_state=random_state)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search.best_params_\n",
    "\n",
    "def cross_validation_with_optimized_hyperparameters(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any], cv_splits: int = 5, random_state: int = 42, model_name: str = 'GaussianNB') -> Dict[str, Any]:\n",
    "    if model_name == 'GaussianNB':\n",
    "        model.var_smoothing = best_params.get('var_smoothing', model.var_smoothing)\n",
    "        scaler = StandardScaler()\n",
    "    elif model_name == 'ComplementNB':\n",
    "        model.alpha = best_params.get('alpha', model.alpha)\n",
    "        model.force_alpha = best_params.get('force_alpha', model.force_alpha)\n",
    "        model.fit_prior = best_params.get('fit_prior', model.fit_prior)\n",
    "        model.norm = best_params.get('norm', model.norm)\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    kfold = KFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    cv_results = cross_validate( X_train, y_train, cv=kfold, return_train_score=True)\n",
    "    return cv_results\n",
    "\n",
    "def initialize_and_split_data(scaler_type: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X, y = initialize_data(scaler_type)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_initial_model(X: np.ndarray, y: np.ndarray, model: Any) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, list]:\n",
    "    mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores = evaluate_model(X, y, model)\n",
    "    logging.info(f\"Mean accuracy score over all folds: {mean_accuracy * 100:.2f}%\")\n",
    "    logging.info(f\"Accuracy for the last fold: {accuracy_scores[-1] * 100:.2f}%\")\n",
    "    return mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports and plots\n",
    "# Function Documentation\n",
    "\n",
    "## `generate_report`\n",
    "\n",
    "Generates a classification report and confusion matrix.\n",
    "\n",
    "### Parameters:\n",
    "- `y_test` (Any): Actual target values.\n",
    "- `y_pred` (Any): Predicted target values.\n",
    "\n",
    "### Returns:\n",
    "- `Dict[str, Any]`: The classification report as a dictionary.\n",
    "\n",
    "## `plot_actual_vs_predicted`\n",
    "\n",
    "Plots actual vs predicted values.\n",
    "\n",
    "### Parameters:\n",
    "- `y_test` (Any): Actual target values.\n",
    "- `y_pred` (Any): Predicted target values.\n",
    "- `ax` (plt.Axes): Matplotlib Axes object to plot on.\n",
    "\n",
    "## `plot_confusion_matrix`\n",
    "\n",
    "Plots the confusion matrix.\n",
    "\n",
    "### Parameters:\n",
    "- `y_test` (Any): Actual target values.\n",
    "- `y_pred` (Any): Predicted target values.\n",
    "- `ax` (plt.Axes): Matplotlib Axes object to plot on.\n",
    "\n",
    "## `plot_classification_report`\n",
    "\n",
    "Plots metrics from a classification report.\n",
    "\n",
    "### Parameters:\n",
    "- `report` (Dict[str, Any]): The classification report as a dictionary.\n",
    "- `ax` (plt.Axes): Matplotlib Axes object to plot on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(y_test: Any, y_pred: Any) -> Dict[str, Any]:\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
    "    logging.info(\"Classification report:\\n%s\", classification_report(y_test, y_pred, zero_division=1))\n",
    "    logging.info(\"Confusion matrix:\\n%s\", confusion_matrix(y_test, y_pred))\n",
    "    return report\n",
    "\n",
    "def plot_actual_vs_predicted(y_test: Any, y_pred: Any, ax: plt.Axes) -> None:\n",
    "    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    test_df.plot(ax=ax, marker='o')\n",
    "    ax.legend(['Actual', 'Predicted'])\n",
    "    ax.set_title('Actual vs Predicted values')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "def plot_confusion_matrix(y_test: Any, y_pred: Any, ax: plt.Axes) -> None:\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "def plot_classification_report(report: Dict[str, Any], ax: plt.Axes) -> None:\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.drop(columns='support', inplace=True)\n",
    "    report_df.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Classification Report Metrics')\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def display_plots(y_fold_test: np.ndarray, y_pred: np.ndarray, report: Dict[str, Any], cv_results: Dict[str, Any], accuracy_without_optimization: float, accuracy_with_optimization: float) -> None:\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    # Plot actual vs predicted values for the last fold\n",
    "    plot_actual_vs_predicted(y_fold_test, y_pred, axs[0, 0])\n",
    "    # Plot confusion matrix for the last fold\n",
    "    plot_confusion_matrix(y_fold_test, y_pred, axs[0, 1])\n",
    "    # Plot classification report for the last fold\n",
    "    plot_classification_report(report, axs[1, 0])\n",
    "    # Plot cross-validation results\n",
    "    ax_cv = axs[1, 1]\n",
    "    ax_cv.plot(cv_results['test_score'], label='Test Score')\n",
    "    ax_cv.plot(cv_results['train_score'], label='Train Score')\n",
    "    ax_cv.set_xlabel('Fold Index')\n",
    "    ax_cv.set_ylabel('Accuracy')\n",
    "    ax_cv.set_title('Cross-validation Results with Optimized Hyperparameters')\n",
    "    ax_cv.legend()\n",
    "    # Plot the accuracy scores comparison\n",
    "    ax_comp = axs[2, 0]\n",
    "    ax_comp.bar(['Without Optimization', 'With Optimization'], [accuracy_without_optimization, accuracy_with_optimization])\n",
    "    ax_comp.set_ylabel('Mean Accuracy Score')\n",
    "    ax_comp.set_title('Comparison of Model Performance With and Without Hyperparameter Optimization')\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes\n",
    "# Function Documentation\n",
    "\n",
    "## `initialize_and_split_data`\n",
    "\n",
    "Initializes data and splits it into training and testing sets.\n",
    "\n",
    "### Parameters:\n",
    "- `scaler_type` (str): The type of scaler to use for preprocessing.\n",
    "\n",
    "### Returns:\n",
    "- `Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]`: Split data (X_train, X_test, y_train, y_test).\n",
    "\n",
    "## `evaluate_initial_model`\n",
    "\n",
    "Evaluates the initial model using cross-validation.\n",
    "\n",
    "### Parameters:\n",
    "- `X` (np.ndarray): Feature data.\n",
    "- `y` (np.ndarray): Target data.\n",
    "- `model` (Any): The machine learning model to evaluate.\n",
    "\n",
    "### Returns:\n",
    "- `Tuple[float, np.ndarray, np.ndarray, np.ndarray, list]`: Evaluation results.\n",
    "\n",
    "## `perform_hyperparameter_optimization`\n",
    "\n",
    "Performs hyperparameter optimization using RandomizedSearchCV.\n",
    "\n",
    "### Parameters:\n",
    "- `X_train` (np.ndarray): Training feature data.\n",
    "- `y_train` (np.ndarray): Training target data.\n",
    "- `model` (Any): The machine learning model to optimize.\n",
    "\n",
    "### Returns:\n",
    "- `Dict[str, Any]`: Best hyperparameters found.\n",
    "\n",
    "## `cross_validate_with_optimized_params`\n",
    "\n",
    "Performs cross-validation with optimized hyperparameters.\n",
    "\n",
    "### Parameters:\n",
    "- `X_train` (np.ndarray): Training feature data.\n",
    "- `y_train` (np.ndarray): Training target data.\n",
    "- `model` (Any): The machine learning model to evaluate.\n",
    "- `best_params` (Dict[str, Any]): Best hyperparameters found.\n",
    "\n",
    "### Returns:\n",
    "- `Dict[str, Any]`: Cross-validation results.\n",
    "\n",
    "## `display_plots`\n",
    "\n",
    "Displays various plots including actual vs predicted, confusion matrix, classification report, cross-validation results, and feature importance.\n",
    "\n",
    "### Parameters:\n",
    "- `y_fold_test` (np.ndarray): Actual target values for the last fold.\n",
    "- `y_pred` (np.ndarray): Predicted target values for the last fold.\n",
    "- `report` (Dict[str, Any]): Classification report for the last fold.\n",
    "- `cv_results` (Dict[str, Any]): Cross-validation results.\n",
    "- `accuracy_without_optimization` (float): Accuracy without hyperparameter optimization.\n",
    "- `accuracy_with_optimization` (float): Accuracy with hyperparameter optimization.\n",
    "- `model` (Any): The trained model for feature importance plotting.\n",
    "\n",
    "## `gaussian_naive_bayes_main`\n",
    "\n",
    "Main function to initialize data, evaluate the model, perform hyperparameter optimization, generate reports, and plot the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 02:23:49,339 - INFO - X shape: (1503, 7), y shape: (1503,)\n",
      "2024-05-30 02:23:49,341 - INFO - Data scaled using standard scaler.\n",
      "2024-05-30 02:23:49,341 - INFO - Data initialization and splitting complete.\n",
      "2024-05-30 02:23:49,345 - INFO - Accuracy for this fold: 49.59%\n",
      "2024-05-30 02:23:49,353 - INFO - Accuracy for this fold: 53.72%\n",
      "2024-05-30 02:23:49,356 - INFO - Accuracy for this fold: 56.67%\n",
      "2024-05-30 02:23:49,358 - INFO - Accuracy for this fold: 51.67%\n",
      "2024-05-30 02:23:49,358 - INFO - Accuracy for this fold: 56.67%\n",
      "2024-05-30 02:23:49,364 - INFO - Accuracy for this fold: 60.83%\n",
      "2024-05-30 02:23:49,365 - INFO - Accuracy for this fold: 55.00%\n",
      "2024-05-30 02:23:49,369 - INFO - Accuracy for this fold: 57.50%\n",
      "2024-05-30 02:23:49,372 - INFO - Accuracy for this fold: 66.67%\n",
      "2024-05-30 02:23:49,375 - INFO - Accuracy for this fold: 53.33%\n",
      "2024-05-30 02:23:49,375 - INFO - Mean accuracy score over all folds: 56.16%\n",
      "2024-05-30 02:23:49,375 - INFO - Accuracy for the last fold: 53.33%\n",
      "2024-05-30 02:23:49,389 - INFO - Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      1.00      0.00         0\n",
      "           4       0.50      0.33      0.40         6\n",
      "           5       0.62      0.69      0.65        48\n",
      "           6       0.48      0.51      0.49        47\n",
      "           7       0.42      0.29      0.34        17\n",
      "           8       1.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.53       120\n",
      "   macro avg       0.50      0.47      0.32       120\n",
      "weighted avg       0.54      0.53      0.52       120\n",
      "\n",
      "2024-05-30 02:23:49,390 - INFO - Confusion matrix:\n",
      "[[ 0  0  0  0  0  0]\n",
      " [ 0  2  4  0  0  0]\n",
      " [ 1  1 33 12  1  0]\n",
      " [ 0  1 16 24  6  0]\n",
      " [ 0  0  0 12  5  0]\n",
      " [ 0  0  0  2  0  0]]\n",
      "2024-05-30 02:23:49,494 - INFO - Best strategy: {'var_smoothing': 1.0}\n",
      "2024-05-30 02:23:49,494 - ERROR - An error occurred during model evaluation and plotting.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\541391404.py\", line 30, in gaussian_naive_bayes_main\n",
      "    cv_results = cross_validate_with_optimized_params(X_train, y_train, gnb, best_strategy)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\541391404.py\", line 8, in cross_validate_with_optimized_params\n",
      "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='GaussianNB')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\3160427771.py\", line 35, in cross_validation_with_optimized_hyperparameters\n",
      "    cv_results = cross_validate( X_train, y_train, cv=kfold, return_train_score=True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 203, in wrapper\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'estimator' parameter of cross_validate must be an object implementing 'fit'. Got array([[-1.38942278,  1.06465996, -1.38584256, ...,  0.09078993,\n",
      "         0.18371536,  0.1403032 ],\n",
      "       [-0.54516392,  0.23144782, -0.60406299, ..., -0.4818109 ,\n",
      "        -0.77479315, -0.52267903],\n",
      "       [ 1.76247697, -0.28930977, -0.3434698 , ...,  0.93348549,\n",
      "         0.55237248, -1.15409068],\n",
      "       ...,\n",
      "       [-0.32002822,  0.02314478, -1.33372392, ..., -0.82753216,\n",
      "         1.14222387,  0.70857368],\n",
      "       [ 0.58051456,  0.33559933, -1.12524937, ..., -0.74650374,\n",
      "        -0.03747891, -0.33325554],\n",
      "       [ 0.24281102, -0.28930977, -0.60406299, ..., -1.14084205,\n",
      "        -0.77479315, -0.61739078]]) instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perform_hyperparameter_optimization(X_train: np.ndarray, y_train: np.ndarray, model: Any) -> Dict[str, Any]:\n",
    "    param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "    best_strategy = optimize_hyperparameters(X_train, y_train, model, param_grid)\n",
    "    logging.info(f\"Best strategy: {best_strategy}\")\n",
    "    return best_strategy\n",
    "\n",
    "def cross_validate_with_optimized_params(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='GaussianNB')\n",
    "    logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "    return cv_results\n",
    "\n",
    "def gaussian_naive_bayes_main():\n",
    "    try:\n",
    "        # Initialize and split data\n",
    "        X_train, X_test, y_train, y_test = initialize_and_split_data(\"standard\")\n",
    "        logging.info(\"Data initialization and splitting complete.\")\n",
    "\n",
    "        # Initial Model Evaluation\n",
    "        gnb = GaussianNB()\n",
    "        mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores = evaluate_initial_model(X_train, y_train, gnb)\n",
    "        accuracy_without_optimization = mean_accuracy\n",
    "\n",
    "        # Report Generation for the Last Fold\n",
    "        report = generate_report(y_fold_test, y_pred)\n",
    "\n",
    "        # Hyperparameter Tuning\n",
    "        best_strategy = perform_hyperparameter_optimization(X_train, y_train, gnb)\n",
    "\n",
    "        # Cross-validation with the optimized hyperparameters\n",
    "        cv_results = cross_validate_with_optimized_params(X_train, y_train, gnb, best_strategy)\n",
    "        accuracy_with_optimization = np.mean(cv_results['test_score'])\n",
    "\n",
    "        # Display plots\n",
    "        display_plots(y_fold_test, y_pred, report, cv_results, accuracy_without_optimization, accuracy_with_optimization)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during model evaluation and plotting.\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaussian_naive_bayes_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 02:23:49,514 - INFO - X shape: (1503, 7), y shape: (1503,)\n",
      "2024-05-30 02:23:49,520 - INFO - Data scaled using minmax scaler.\n",
      "2024-05-30 02:23:49,522 - INFO - Data initialization and splitting complete.\n",
      "2024-05-30 02:23:49,525 - INFO - Accuracy for this fold: 48.76%\n",
      "2024-05-30 02:23:49,529 - INFO - Accuracy for this fold: 54.55%\n",
      "2024-05-30 02:23:49,531 - INFO - Accuracy for this fold: 52.50%\n",
      "2024-05-30 02:23:49,534 - INFO - Accuracy for this fold: 50.00%\n",
      "2024-05-30 02:23:49,536 - INFO - Accuracy for this fold: 51.67%\n",
      "2024-05-30 02:23:49,539 - INFO - Accuracy for this fold: 55.00%\n",
      "2024-05-30 02:23:49,539 - INFO - Accuracy for this fold: 53.33%\n",
      "2024-05-30 02:23:49,544 - INFO - Accuracy for this fold: 53.33%\n",
      "2024-05-30 02:23:49,544 - INFO - Accuracy for this fold: 57.50%\n",
      "2024-05-30 02:23:49,544 - INFO - Accuracy for this fold: 53.33%\n",
      "2024-05-30 02:23:49,544 - INFO - Mean accuracy score over all folds: 53.00%\n",
      "2024-05-30 02:23:49,551 - INFO - Accuracy for the last fold: 53.33%\n",
      "2024-05-30 02:23:49,555 - INFO - Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.00      0.00         6\n",
      "           5       0.58      0.75      0.65        48\n",
      "           6       0.47      0.55      0.51        47\n",
      "           7       0.67      0.12      0.20        17\n",
      "           8       1.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.53       120\n",
      "   macro avg       0.74      0.28      0.27       120\n",
      "weighted avg       0.58      0.53      0.49       120\n",
      "\n",
      "2024-05-30 02:23:49,565 - INFO - Confusion matrix:\n",
      "[[ 0  5  1  0  0]\n",
      " [ 0 36 11  1  0]\n",
      " [ 0 21 26  0  0]\n",
      " [ 0  0 15  2  0]\n",
      " [ 0  0  2  0  0]]\n",
      "2024-05-30 02:23:49,670 - INFO - Best strategy: {'norm': True, 'force_alpha': True, 'fit_prior': True, 'alpha': 6.892612104349702}\n",
      "2024-05-30 02:23:49,671 - ERROR - An error occurred during model evaluation and plotting.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\3075975146.py\", line 36, in complement_naive_bayes_main\n",
      "    cv_results = cross_validate_with_optimized_params(X_train, y_train, cnb, best_strategy)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\3075975146.py\", line 13, in cross_validate_with_optimized_params\n",
      "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='ComplementNB')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\3160427771.py\", line 35, in cross_validation_with_optimized_hyperparameters\n",
      "    cv_results = cross_validate( X_train, y_train, cv=kfold, return_train_score=True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 203, in wrapper\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'estimator' parameter of cross_validate must be an object implementing 'fit'. Got array([[0.1322314 , 0.59493671, 0.25609756, ..., 0.51713633, 0.38202247,\n",
      "        0.39285714],\n",
      "       [0.25619835, 0.39240506, 0.34756098, ..., 0.43640518, 0.23595506,\n",
      "        0.26785714],\n",
      "       [0.59504132, 0.26582278, 0.37804878, ..., 0.63594821, 0.43820225,\n",
      "        0.14880952],\n",
      "       ...,\n",
      "       [0.2892562 , 0.34177215, 0.26219512, ..., 0.38766184, 0.52808989,\n",
      "        0.5       ],\n",
      "       [0.4214876 , 0.41772152, 0.28658537, ..., 0.39908606, 0.34831461,\n",
      "        0.30357143],\n",
      "       [0.37190083, 0.26582278, 0.34756098, ..., 0.34348819, 0.23595506,\n",
      "        0.25      ]]) instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perform_hyperparameter_optimization(X_train: np.ndarray, y_train: np.ndarray, model: Any) -> Dict[str, Any]:\n",
    "    param_grid = {\n",
    "            'alpha': np.logspace(-3, 1, num=100),\n",
    "            'force_alpha': [True, False],\n",
    "            'fit_prior': [True, False],\n",
    "            'norm': [True, False]\n",
    "        }\n",
    "    best_strategy = optimize_hyperparameters(X_train, y_train, model, param_grid)\n",
    "    logging.info(f\"Best strategy: {best_strategy}\")\n",
    "    return best_strategy\n",
    "\n",
    "def cross_validate_with_optimized_params(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='ComplementNB')\n",
    "    logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def complement_naive_bayes_main():\n",
    "    try:\n",
    "        # Initialize and split data\n",
    "        X_train, X_test, y_train, y_test = initialize_and_split_data(\"minmax\")\n",
    "        logging.info(\"Data initialization and splitting complete.\")\n",
    "\n",
    "        # Initial Model Evaluation\n",
    "        cnb = ComplementNB()\n",
    "        mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores = evaluate_initial_model(X_train, y_train, cnb)\n",
    "        accuracy_without_optimization = mean_accuracy\n",
    "\n",
    "        # Report Generation for the Last Fold\n",
    "        report = generate_report(y_fold_test, y_pred)\n",
    "\n",
    "        # Hyperparameter Tuning\n",
    "        best_strategy = perform_hyperparameter_optimization(X_train, y_train, cnb)\n",
    "\n",
    "        # Cross-validation with the optimized hyperparameters\n",
    "        cv_results = cross_validate_with_optimized_params(X_train, y_train, cnb, best_strategy)\n",
    "        accuracy_with_optimization = np.mean(cv_results['test_score'])\n",
    "\n",
    "        # Display plots\n",
    "        display_plots(y_fold_test, y_pred, report, cv_results, accuracy_without_optimization, accuracy_with_optimization)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during model evaluation and plotting.\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    complement_naive_bayes_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 02:23:49,688 - INFO - X shape: (1503, 7), y shape: (1503,)\n",
      "2024-05-30 02:23:49,691 - INFO - Data scaled using minmax scaler.\n",
      "2024-05-30 02:23:49,692 - INFO - Data initialization and splitting complete.\n",
      "2024-05-30 02:23:49,694 - INFO - Accuracy for this fold: 48.76%\n",
      "2024-05-30 02:23:49,697 - INFO - Accuracy for this fold: 57.85%\n",
      "2024-05-30 02:23:49,699 - INFO - Accuracy for this fold: 59.17%\n",
      "2024-05-30 02:23:49,701 - INFO - Accuracy for this fold: 55.00%\n",
      "2024-05-30 02:23:49,703 - INFO - Accuracy for this fold: 53.33%\n",
      "2024-05-30 02:23:49,706 - INFO - Accuracy for this fold: 52.50%\n",
      "2024-05-30 02:23:49,708 - INFO - Accuracy for this fold: 53.33%\n",
      "2024-05-30 02:23:49,710 - INFO - Accuracy for this fold: 61.67%\n",
      "2024-05-30 02:23:49,712 - INFO - Accuracy for this fold: 59.17%\n",
      "2024-05-30 02:23:49,714 - INFO - Accuracy for this fold: 50.83%\n",
      "2024-05-30 02:23:49,714 - INFO - Mean accuracy score over all folds: 55.16%\n",
      "2024-05-30 02:23:49,715 - INFO - Accuracy for the last fold: 50.83%\n",
      "2024-05-30 02:23:49,726 - INFO - Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.00      0.00         6\n",
      "           5       0.54      0.81      0.65        48\n",
      "           6       0.46      0.47      0.46        47\n",
      "           7       1.00      0.00      0.00        17\n",
      "           8       1.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.51       120\n",
      "   macro avg       0.80      0.26      0.22       120\n",
      "weighted avg       0.60      0.51      0.44       120\n",
      "\n",
      "2024-05-30 02:23:49,727 - INFO - Confusion matrix:\n",
      "[[ 0  5  1  0  0]\n",
      " [ 0 39  9  0  0]\n",
      " [ 0 25 22  0  0]\n",
      " [ 0  3 14  0  0]\n",
      " [ 0  0  2  0  0]]\n",
      "2024-05-30 02:23:49,815 - INFO - Best strategy: {'force_alpha': False, 'fit_prior': True, 'alpha': 0.1261856883066021}\n",
      "2024-05-30 02:23:49,817 - ERROR - An error occurred during model evaluation and plotting.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\188091080.py\", line 35, in multinominal_naive_bayes_main\n",
      "    cv_results = cross_validate_with_optimized_params(X_train, y_train, mnb, best_strategy)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\188091080.py\", line 12, in cross_validate_with_optimized_params\n",
      "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='MultinomialNB')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_13348\\3160427771.py\", line 35, in cross_validation_with_optimized_hyperparameters\n",
      "    cv_results = cross_validate( X_train, y_train, cv=kfold, return_train_score=True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 203, in wrapper\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'estimator' parameter of cross_validate must be an object implementing 'fit'. Got array([[0.1322314 , 0.59493671, 0.25609756, ..., 0.51713633, 0.38202247,\n",
      "        0.39285714],\n",
      "       [0.25619835, 0.39240506, 0.34756098, ..., 0.43640518, 0.23595506,\n",
      "        0.26785714],\n",
      "       [0.59504132, 0.26582278, 0.37804878, ..., 0.63594821, 0.43820225,\n",
      "        0.14880952],\n",
      "       ...,\n",
      "       [0.2892562 , 0.34177215, 0.26219512, ..., 0.38766184, 0.52808989,\n",
      "        0.5       ],\n",
      "       [0.4214876 , 0.41772152, 0.28658537, ..., 0.39908606, 0.34831461,\n",
      "        0.30357143],\n",
      "       [0.37190083, 0.26582278, 0.34756098, ..., 0.34348819, 0.23595506,\n",
      "        0.25      ]]) instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perform_hyperparameter_optimization(X_train: np.ndarray, y_train: np.ndarray, model: Any) -> Dict[str, Any]:\n",
    "    param_grid = {\n",
    "            'alpha': np.logspace(-3, 1, num=100),\n",
    "            'force_alpha': [True, False],\n",
    "            'fit_prior': [True, False],\n",
    "        }\n",
    "    best_strategy = optimize_hyperparameters(X_train, y_train, model, param_grid)\n",
    "    logging.info(f\"Best strategy: {best_strategy}\")\n",
    "    return best_strategy\n",
    "\n",
    "def cross_validate_with_optimized_params(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='MultinomialNB')\n",
    "    logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def multinominal_naive_bayes_main():\n",
    "    try:\n",
    "        # Initialize and split data\n",
    "        X_train, X_test, y_train, y_test = initialize_and_split_data(\"minmax\")\n",
    "        logging.info(\"Data initialization and splitting complete.\")\n",
    "\n",
    "        # Initial Model Evaluation\n",
    "        mnb = MultinomialNB()\n",
    "        mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores = evaluate_initial_model(X_train, y_train, mnb)\n",
    "        accuracy_without_optimization = mean_accuracy\n",
    "\n",
    "        # Report Generation for the Last Fold\n",
    "        report = generate_report(y_fold_test, y_pred)\n",
    "\n",
    "        # Hyperparameter Tuning\n",
    "        best_strategy = perform_hyperparameter_optimization(X_train, y_train, mnb)\n",
    "\n",
    "        # Cross-validation with the optimized hyperparameters\n",
    "        cv_results = cross_validate_with_optimized_params(X_train, y_train, mnb, best_strategy)\n",
    "        accuracy_with_optimization = np.mean(cv_results['test_score'])\n",
    "\n",
    "        # Display plots\n",
    "        display_plots(y_fold_test, y_pred, report, cv_results, accuracy_without_optimization, accuracy_with_optimization)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during model evaluation and plotting.\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multinominal_naive_bayes_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
