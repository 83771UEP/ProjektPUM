{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model lasy drzew decyzyjnych\n",
    "\n",
    "### 1. Argumenty za przyjęciem modelu lasu drzew decyzyjnych:\n",
    "- zdolność radzenia sobie z dużą liczbą cech\n",
    "- ddolność uwzględniania nieliniowych relacji\n",
    "- duża odporność na nadmierne dopasowanie\n",
    "- łatwość interpretacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import bibliotek oraz danych z plików .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV, cross_validate, train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import export_graphviz, plot_tree\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(scaler_type: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    # Load data from CSV files\n",
    "    X = pd.read_csv('wineData.csv')\n",
    "    y = pd.read_csv('wineRatings.csv')\n",
    "    y = y.values.ravel()\n",
    "    \n",
    "    # Log data information\n",
    "    logging.info(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    # Preprocess data using the specified scaler\n",
    "    if scaler_type in [\"minmax\", \"standard\"]:\n",
    "        scaler = MinMaxScaler() if scaler_type == \"minmax\" else StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        logging.info(f\"Data scaled using {scaler_type} scaler.\")\n",
    "    else:\n",
    "        logging.info(\"No scaling applied.\")\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Narysowanie histogramów opisujących częstotliwości występowania poziomów każdej cechy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #definicja metody rysującej diagramy dla kazdej cechy\n",
    "# def draw_frequency_diagram(X_train, X_test):\n",
    "\n",
    "#     #Do celów prezentacji danych łączymy tymaczasowo dane treningowe i testowe w jednym DataFrame\n",
    "#     X_combined = pd.concat([X_train, X_test], ignore_index=True)\n",
    "\n",
    "#     num_columns = X_combined.shape[1]\n",
    "\n",
    "#     # Narysuj histogram\n",
    "#     for i in range(num_columns):\n",
    "#         column_data = X_combined[X_combined.columns[i]] \n",
    "#         plt.hist(column_data, bins=20, color='skyblue', edgecolor='black')\n",
    "#         plt.xlabel(X_combined.columns[i])\n",
    "#         plt.ylabel('Częstotliwość')\n",
    "#         plt.title(f'Histogram dla {X_combined.columns[i]}')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sampling_strategy(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    return {class_label: max(int(0.10 * max_count), count) for class_label, count in zip(unique, counts)}\n",
    "\n",
    "def initialize_and_split_data(scaler_type: str, method: str = 'none') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X, y = initialize_data(scaler_type)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    if method == 'smote':\n",
    "        strategy = sampling_strategy(y_train)\n",
    "        oversampler = SMOTE(sampling_strategy=strategy, k_neighbors=2, random_state=42)\n",
    "        X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "        logging.info(f\"Applied {method} to handle class imbalance in the training data.\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate_model(X_train: np.ndarray, y_train: np.ndarray, model: Any, cv_splits: int = 5, random_state: int = 2137) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, list]:\n",
    "    kfold = KFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train[train_index], X_train[test_index]\n",
    "        y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred = model.predict(X_fold_test)\n",
    "        accuracy = accuracy_score(y_fold_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        logging.info(f\"Accuracy for this fold: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    mean_accuracy = np.mean(accuracy_scores)\n",
    "    return mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores\n",
    "\n",
    "def optimize_hyperparameters(X_train: np.ndarray, y_train: np.ndarray, estimator: Any, param_grid: Dict[str, Any], n_iter: int = 100, cv_splits: int = 5, random_state: int = 42) -> Dict[str, Any]:\n",
    "    random_search = RandomizedSearchCV(estimator, param_distributions=param_grid, n_iter=n_iter, cv=cv_splits, scoring='accuracy', random_state=random_state)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    logging.info(f\"Best accuracy score: {random_search.best_score_ * 100:.2f}%\")\n",
    "    return random_search.best_params_\n",
    "\n",
    "def cross_validation_with_optimized_hyperparameters(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any], cv_splits: int = 5, random_state: int = 42, model_name: str = 'GaussianNB') -> Dict[str, Any]:\n",
    "    model.set_params(**best_params)\n",
    "    pipe = make_pipeline(model)\n",
    "    kfold = KFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, cv=kfold, return_train_score=True)\n",
    "    return cv_results\n",
    "\n",
    "def evaluate_initial_model(X: np.ndarray, y: np.ndarray, model: Any) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, list]:\n",
    "    mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores = evaluate_model(X, y, model)\n",
    "    logging.info(f\"Mean accuracy score over all folds: {mean_accuracy * 100:.2f}%\")\n",
    "    logging.info(f\"Accuracy for the last fold: {accuracy_scores[-1] * 100:.2f}%\")\n",
    "    return mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_report(y_test: Any, y_pred: Any) -> Dict[str, Any]:\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
    "    logging.info(\"Classification report:\\n%s\", classification_report(y_test, y_pred, zero_division=1))\n",
    "    logging.info(\"Confusion matrix:\\n%s\", confusion_matrix(y_test, y_pred))\n",
    "    return report\n",
    "\n",
    "def plot_actual_vs_predicted(y_test: Any, y_pred: Any, ax: plt.Axes) -> None:\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).plot(ax=ax, marker='o')\n",
    "    ax.set_title('Actual vs Predicted values')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.legend(['Actual', 'Predicted'])\n",
    "\n",
    "def plot_confusion_matrix(y_test: Any, y_pred: Any, ax: plt.Axes) -> None:\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "def plot_classification_report(report: Dict[str, Any], ax: plt.Axes) -> None:\n",
    "    report_df = pd.DataFrame(report).transpose().drop(columns='support')\n",
    "    report_df.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Classification Report Metrics')\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def display_plots(y_fold_test: np.ndarray, y_pred: np.ndarray, report: Dict[str, Any], cv_results: Dict[str, Any], accuracy_without_optimization: float, accuracy_with_optimization: float) -> None:\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    \n",
    "    plot_actual_vs_predicted(y_fold_test, y_pred, axs[0, 0])\n",
    "    plot_confusion_matrix(y_fold_test, y_pred, axs[0, 1])\n",
    "    plot_classification_report(report, axs[1, 0])\n",
    "    \n",
    "    axs[1, 1].plot(cv_results['test_score'], label='Test Score')\n",
    "    axs[1, 1].plot(cv_results['train_score'], label='Train Score')\n",
    "    axs[1, 1].set_title('Cross-validation Results with Optimized Hyperparameters')\n",
    "    axs[1, 1].set_xlabel('Fold Index')\n",
    "    axs[1, 1].set_ylabel('Accuracy')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[2, 0].bar(['Without Optimization', 'With Optimization'], [accuracy_without_optimization, accuracy_with_optimization])\n",
    "    axs[2, 0].set_title('Comparison of Model Performance')\n",
    "    axs[2, 0].set_ylabel('Mean Accuracy Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 21:30:05,435 - INFO - X shape: (1503, 5), y shape: (1503,)\n",
      "2024-05-30 21:30:05,444 - INFO - Data scaled using standard scaler.\n",
      "2024-05-30 21:30:05,445 - INFO - Data initialization and splitting complete.\n",
      "2024-05-30 21:30:05,675 - INFO - Accuracy for this fold: 72.51%\n",
      "2024-05-30 21:30:05,886 - INFO - Accuracy for this fold: 60.66%\n",
      "2024-05-30 21:30:06,085 - INFO - Accuracy for this fold: 64.76%\n",
      "2024-05-30 21:30:06,285 - INFO - Accuracy for this fold: 62.38%\n",
      "2024-05-30 21:30:06,476 - INFO - Accuracy for this fold: 65.24%\n",
      "2024-05-30 21:30:06,485 - INFO - Mean accuracy score over all folds: 65.11%\n",
      "2024-05-30 21:30:06,485 - INFO - Accuracy for the last fold: 65.24%\n",
      "2024-05-30 21:30:06,495 - INFO - Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       0.71      0.76      0.73        95\n",
      "           6       0.61      0.64      0.62        84\n",
      "           7       0.58      0.48      0.52        23\n",
      "           8       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.65       210\n",
      "   macro avg       0.58      0.38      0.38       210\n",
      "weighted avg       0.63      0.65      0.64       210\n",
      "\n",
      "2024-05-30 21:30:06,495 - INFO - Confusion matrix:\n",
      "[[ 0  4  3  0  0]\n",
      " [ 1 72 21  1  0]\n",
      " [ 0 23 54  7  0]\n",
      " [ 0  2 10 11  0]\n",
      " [ 0  0  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def perform_hyperparameter_optimization(X_train: np.ndarray, y_train: np.ndarray, model: Any) -> Dict[str, Any]:\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    best_params = optimize_hyperparameters(X_train, y_train, model, param_grid)\n",
    "    logging.info(f\"Best parameters: {best_params}\")\n",
    "    return best_params\n",
    "\n",
    "def cross_validate_with_optimized_params(X_train: np.ndarray, y_train: np.ndarray, model: Any, best_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    cv_results = cross_validation_with_optimized_hyperparameters(X_train, y_train, model, best_params, model_name='RandomForest')\n",
    "    logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "    return cv_results\n",
    "\n",
    "def random_forest_main():\n",
    "    try:\n",
    "        # Initialize and split data\n",
    "        X_train, X_test, y_train, y_test = initialize_and_split_data(\"standard\", 'none')\n",
    "        logging.info(\"Data initialization and splitting complete.\")\n",
    "\n",
    "        # Initial Model Evaluation\n",
    "        rf = RandomForestClassifier()\n",
    "        mean_accuracy, X_fold_test, y_fold_test, y_pred, accuracy_scores = evaluate_initial_model(X_train, y_train, rf)\n",
    "        accuracy_without_optimization = mean_accuracy\n",
    "\n",
    "        # Report Generation for the Last Fold\n",
    "        report = generate_report(y_fold_test, y_pred)\n",
    "\n",
    "        # Hyperparameter Tuning\n",
    "        best_params = perform_hyperparameter_optimization(X_train, y_train, rf)\n",
    "\n",
    "        # Cross-validation with the optimized hyperparameters\n",
    "        cv_results = cross_validate_with_optimized_params(X_train, y_train, rf, best_params)\n",
    "        accuracy_with_optimization = np.mean(cv_results['test_score'])\n",
    "\n",
    "        # Display plots\n",
    "        display_plots(y_fold_test, y_pred, report, cv_results, accuracy_without_optimization, accuracy_with_optimization)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during model evaluation and plotting.\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random_forest_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_combined = pd.concat([y_train, y_test], ignore_index=True)\n",
    "class_names = Y_combined['quality'].unique().tolist()\n",
    "class_names_str = list(map(str, class_names)) \n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(len(rf.estimators_)):\n",
    "    plt.subplot(1, len(rf.estimators_), i + 1)\n",
    "    plot_tree(rf.estimators_[i], feature_names=X_test.columns.tolist(), class_names=class_names_str, filled=True)\n",
    "    plt.title(f'Decision Tree {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
